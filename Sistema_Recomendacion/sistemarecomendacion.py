# -*- coding: utf-8 -*-
"""SistemaRecomendacion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oCCJEFcbbWSXtdEsBIsxp3hF7R9hmKzt
"""

# Cargamos las librerías que utilizaremos
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import sklearn

df_users = pd.read_csv("https://raw.githubusercontent.com/JhonDanielGonzalezLopez/datos/main/users.csv")
df_repos = pd.read_csv("https://raw.githubusercontent.com/JhonDanielGonzalezLopez/datos/main/repos.csv")
df_ratings = pd.read_csv("https://raw.githubusercontent.com/JhonDanielGonzalezLopez/datos/main/ratings.csv")
print(df_users.head())
print(df_repos.head())
print(df_ratings.head())

# Vemos que es un dataset reducido, pequeño. Tenemos 30 usuarios y 167 repositorios valorados.
n_users = df_ratings.userId.unique().shape[0]
n_items = df_ratings.repoId.unique().shape[0]
print (str(n_users) + ' users')
print (str(n_items) + ' items')

# Tenemos más de 80 valoraciones con una puntuación de 1 y unas 40 con puntuación en 5.
plt.hist(df_ratings.rating,bins=8)

#  Veamos las cantidades exactas:
df_ratings.groupby(["rating"])["userId"].count()

#Aquí vemos la cantidad de repositorios y cuantos usuarios “los tienen”. La mayoría de repos los tiene 1 sólo usuario, y no los demás. Hay unos 30 que los tienen 2 usuarios y unos 20 que coinciden 3 usuarios. La suma total debe dar 167.
plt.hist(df_ratings.groupby(["repoId"])["repoId"].count(),bins=8)

# Ahora crearemos la matriz en la que cruzamos todos los usuarios con todos los repositorios.
df_matrix = pd.pivot_table(df_ratings, values='rating', index='userId', columns='repoId').fillna(0)
df_matrix

# Veamos el porcentaje de sparcity que tenemos:
ratings = df_matrix.values
sparsity = float(len(ratings.nonzero()[0]))
sparsity /= (ratings.shape[0] * ratings.shape[1])
sparsity *= 100
print('Sparsity: {:4.2f}%'.format(sparsity))

# Esto serán muchos “ceros” que rellenar (predecir)…
# Separamos en train y test para -más adelante- poder medir la calidad de nuestras recomendaciones.
ratings_train, ratings_test = train_test_split(ratings, test_size = 0.2, random_state=42)
print(ratings_train.shape)
print(ratings_test.shape)

# Matriz de Similitud: Distancias por Coseno
# Ahora calculamos en una nueva matriz la similitud entre usuarios.
sim_matrix = 1 - sklearn.metrics.pairwise.cosine_distances(ratings)
print(sim_matrix.shape)

# Cuanto más cercano a 1, mayor similitud entre esos usuarios
plt.imshow(sim_matrix);
plt.colorbar()
plt.show()

# Predicciones ó Sugeridos
# separar las filas y columnas de train y test
sim_matrix_train = sim_matrix[0:24,0:24]
sim_matrix_test = sim_matrix[24:30,24:30]
 
users_predictions = sim_matrix_train.dot(ratings_train) / np.array([np.abs(sim_matrix_train).sum(axis=1)]).T

# Vemos pocas recomendaciones que logren puntuar alto. La mayoría estará entre 1 y 2 puntos. Esto tiene que ver con nuestro dataset pequeño.
plt.rcParams['figure.figsize'] = (20.0, 5.0)
plt.imshow(users_predictions);
plt.colorbar()
plt.show()

# Vamos a tomar de ejemplo el usuario de Github jbagnato.
#jbagnato
USUARIO_EJEMPLO = 'jbagnato'
data = df_users[df_users['username'] == USUARIO_EJEMPLO]
usuario_ver = data.iloc[0]['userId'] - 1 # resta 1 para obtener el index de pandas.

user0=users_predictions.argsort()[usuario_ver]
 
# Veamos los tres recomendados con mayor puntaje en la predic para este usuario
# Vemos que los tres repositorios con mayor puntaje para sugerir a mi usuario son el de Data-Science–Cheat-Sheet con una puntuación de 3.36, practical-machine-learning-with-py con 2.44 y youtube-dl con 2.06. Lo cierto es que no son puntuaciones muy altas, pero tiene que ver con que la base de datos (nuestro csv) tiene muy pocos repositorios y usuarios cargados.
for i, aRepo in enumerate(user0[-3:]):
    selRepo = df_repos[df_repos['repoId']==(aRepo+1)]
    print(selRepo['title'] , 'puntaje:', users_predictions[usuario_ver][aRepo])

# Validemos el error
# Sobre el test set comparemos el mean squared error con el conjunto de entrenamiento:
def get_mse(preds, actuals):
    if preds.shape[1] != actuals.shape[1]:
        actuals = actuals.T
    preds = preds[actuals.nonzero()].flatten()
    actuals = actuals[actuals.nonzero()].flatten()
    return mean_squared_error(preds, actuals)
 
get_mse(users_predictions, ratings_train)
 
# Realizo las predicciones para el test set
# Vemos que para el conjunto de train y test el MAE es bastante cercano. Un indicador de que no tiene buenas predicciones sería si el MAE en test fuera 2 veces más (ó la mitad) del valor del de train.
users_predictions_test = sim_matrix.dot(ratings) / np.array([np.abs(sim_matrix).sum(axis=1)]).T
users_predictions_test = users_predictions_test[24:30,:]
 
get_mse(users_predictions_test, ratings_test)